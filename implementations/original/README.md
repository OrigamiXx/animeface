
# original

Original models.  
(They might exist and I just don't know about it.)

## Models

- SEBigGAN

    Use Squeeze and Excitation Network instead of Self-Attention layer in BigGAN.

## Author

[Tomoya Sawada](https://github.com/STomoya)
